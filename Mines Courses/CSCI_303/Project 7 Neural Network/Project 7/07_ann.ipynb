{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 7: Facial Detection with Artificial Neural Networks\n",
    "- Name: Carson Stevens\n",
    "- Date: November 28, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "Facial detection is a specific case of object-class detection, which attempts to find the location and sizes of objects in an image that belong to a given class. We're going to train a neural network to do this by giving it a database of positive examples so it can learn what to look for and negative examples so it can learn what not to look for.\n",
    "\n",
    "There are multiple ways to do this, which you can read more about [here](https://towardsdatascience.com/face-detection-for-beginners-e58e8f21aad9), but here we're going to take a feature-based approach: the structural features of each face will be extracted, then the model is trained to spot them and acts as a classifier.\n",
    "\n",
    "Facial detection is very useful for camera-based apps, which need to find faces quickly. One \"quick and dirty\" method is to train a model with HaaR features - the sums of pixel intensities in adjacent regions of a photo. This is fast and pretty successful, but trades some accuracy for time. For an example, look [here](https://docs.opencv.org/3.4.3/d7/d8b/tutorial_py_face_detection.html)\n",
    "\n",
    "We're going to be using a technique known as Histogram of Oriented Gradients, which is described more fully below. It's an older method, but remains the state of the art of high accuracy techniques. It's a lot slower than HaaR (and definitely too slow for cameras), but what we lose in time, we gain in predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import skimage.data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from skimage import color, feature\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from skimage import data, color, feature, transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.image import PatchExtractor\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "# ignore warnings - they're just of future deprecations\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: load the dataset (25 pts)\n",
    "For our faces, we're going to the [Labeled Faces in the Wild(LFW)](http://vis-www.cs.umass.edu/lfw/) database. It has tens of thousands of both positive and negative samples and is usually used for facial *recognition*, which involves teaching our model names as well as the facial features that go along with that name, but we're just going to detect here.\n",
    "\n",
    "- Load the faces with `faces = fetch_lfw_people()` then save the images in a variable from  `faces.images`. These 'patches' will be our positive training samples.\n",
    "\n",
    "- Next, negative patches are loaded for you.\n",
    "\n",
    "- Combine positive and negative patches into a single set of inputs. (Hint: consider `np.concatenate()`)\n",
    "\n",
    "- Finally, create a set of targets with a `1` for every positive patch and a `0` for every negative. (Hint: consider `np.ones()` and `np.zeros()`) You can get the number of both sets of patches with `shape[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "## positive\n",
    "faces = fetch_lfw_people()\n",
    "positive_patches = faces.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## negative patches\n",
    "imgs_to_use = ['camera', 'text', 'coins', 'moon',\n",
    "'page', 'clock', 'immunohistochemistry',\n",
    "'chelsea', 'coffee', 'hubble_deep_field']\n",
    "\n",
    "negative_images = [color.rgb2gray(getattr(data, name)()) for name in imgs_to_use]\n",
    "\n",
    "def extract_patches(img, N, scale=1.0, patch_size=positive_patches[0].shape):\n",
    "    extracted_patch_size = tuple((scale * np.array(patch_size)).astype(int))\n",
    "    extractor = PatchExtractor(patch_size=extracted_patch_size,\n",
    "                               max_patches=N, random_state=0)\n",
    "    patches = extractor.transform(img[np.newaxis])\n",
    "    if scale != 1:\n",
    "        patches = np.array([transform.resize(patch, patch_size)\n",
    "                            for patch in patches])\n",
    "    return patches\n",
    "\n",
    "negative_patches = np.vstack([extract_patches(im, 500, scale) for im in negative_images for scale in [0.5, 1.0, 2.0]])\n",
    "negative_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine positive and negative\n",
    "# inputs\n",
    "X = np.concatenate((positive_patches, negative_patches), axis=0)\n",
    "# targets\n",
    "y = np.concatenate((np.ones(len(positive_patches)), np.zeros(len(negative_patches))))\n",
    "#print(X.shape, y.shape, positive_patches.shape, negative_patches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: HOG features (10 pts)\n",
    "*Histogram of Oriented Gradients* is a feature descriptor, which is a representation of an image that simplifies it by extracting useful information and removes extraneous information. This feature descriptor counts occurrences of \"gradient orientation\" in localized portions of an image. Basically, an image is divided into regions. In each region, the directions and extremities of light intensity or edges are calculated. All these different gradients are then compiled into a histogram. Then histograms from multiple regions can be normalized by measuring intensity across multiple regions together. The information extracted here can be used for face detection by feeding it to a learning model to teach it how light interacts with faces and the edges and shapes parts of the face has.\n",
    "\n",
    "Here are some more resources to learn about HOG\n",
    "- [wiki](https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients)\n",
    "- [OpenCV blog](https://www.learnopencv.com/histogram-of-oriented-gradients/)\n",
    "- [Stanford](http://vision.stanford.edu/teaching/cs231b_spring1213/slides/HOG_2011_Stanford.pdf)\n",
    "- [Intel](https://software.intel.com/en-us/ipp-dev-reference-histogram-of-oriented-gradients-hog-descriptor)\n",
    "\n",
    "**Procedure**<br>\n",
    "We want to call `skimage.feature.hog()` on each image, either in a loop or a comprehension, store each in a list, then create an `np.array` from that list.\n",
    "\n",
    "Then, split that array with `train_test_split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hog features\n",
    "hog_X = [skimage.feature.hog(face) for face in X]\n",
    "hog_X = np.array(hog_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(hog_X, y)\n",
    "#print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image functions - these functions are given and will be used below ... leave as is please :)\n",
    "def show_img(img, interpolation = 'bicubic'):\n",
    "    '''\n",
    "    basic image plotter\n",
    "\n",
    "    Arguments:\n",
    "        img - the image(which is an ndarray)\n",
    "\n",
    "    Returns:\n",
    "        void\n",
    "    '''\n",
    "    plt.imshow(img, cmap='gray', interpolation = interpolation)\n",
    "    plt.axis('off') if not axis else None\n",
    "    plt.show()\n",
    "    \n",
    "def convert_img_to_greyscale(img):\n",
    "    '''\n",
    "    convert an image to greyscale\n",
    "\n",
    "    Arguments:\n",
    "    img - the image/ndarray\n",
    "\n",
    "    Returns: \n",
    "    '''\n",
    "    return skimage.color.rgb2gray(img)\n",
    "\n",
    "def rescale_img(img, r_scale=0.5, r_mode='reflect'):\n",
    "    '''\n",
    "    rescale inage\n",
    "    \n",
    "    Arguments:\n",
    "        img - the image/ndarray\n",
    "        r_scale - \n",
    "        r_mode - \n",
    "\n",
    "    Returns:\n",
    "        rescaled image - skimage\n",
    "    '''\n",
    "    return skimage.transform.rescale(img, r_scale, mode=r_mode)\n",
    "\n",
    "def show_hog_features(image, is_greyscale=False, is_rescaled=False):\n",
    "    '''\n",
    "    plot image and hog features side by side\n",
    "    \n",
    "    Arguments:\n",
    "       image - the image\n",
    "       is_greyscale - \n",
    "       is_rescaled - \n",
    "    '''\n",
    "    if not is_greyscale:\n",
    "        image = convert_img_to_greyscale(image)\n",
    "    if not is_rescaled:  \n",
    "        image = rescale_img(image)\n",
    "    hog_vec, hog_vis = feature.hog(image, visualise=True)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6), subplot_kw=dict(xticks=[], yticks=[]))\n",
    "    ax[0].imshow(image, cmap='gray')\n",
    "    ax[0].set_title('input image')\n",
    "\n",
    "    ax[1].imshow(hog_vis, cmap='gray')\n",
    "    ax[1].set_title('visualization of HOG features');\n",
    "    plt.show()\n",
    "    \n",
    "def show_patches(img, indices, label):\n",
    "    '''\n",
    "    superimpose the patches predicted to contain a face on the original image\n",
    "    \n",
    "    Arguments:\n",
    "        img - the image\n",
    "        indices - \n",
    "        label - \n",
    "        \n",
    "    Returns:\n",
    "        void\n",
    "    '''\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "    Ni, Nj = positive_patches[0].shape\n",
    "    indices_arr = np.array(indices)\n",
    "\n",
    "    for i, j in indices_arr[label == 1]:\n",
    "         ax.add_patch(plt.Rectangle((j, i), Nj, Ni, edgecolor='red', alpha=0.3, lw=2, facecolor='none'))\n",
    "    \n",
    "    plt.title(\"Faces Found: \" + str(len(indices_arr[label == 1])))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example visualization of HOG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These code is also given to load the images we will be working with and the HOG result\n",
    "astronaut = skimage.data.astronaut()\n",
    "show_hog_features(astronaut)\n",
    "cropped_astronaut = rescale_img(convert_img_to_greyscale(astronaut))[0:100, 70:160]\n",
    "show_hog_features(cropped_astronaut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Multilayer Perceptron (MLP) (10 pts)\n",
    "A perceptron is a linear classifier - it classifies input by separating two categories with the straight line: `y = w * X + b`,  the input feature vector `X` is multipled by weights `w` with an added bias `b`. This \"neuron\" works by calculating a weighted sum of its input, adds the bias, and decides whether it should \"fire\" or not. By itself, a perceptron is just a building block, only useful when combined and expanded into larger functions, such as a multilayer perceptron.\n",
    "\n",
    "An MLP is a deep artificial neural network, differing from single-hidden-layer neural networks in their *depth* - how many layers of nodes data passes through in the process of pattern recognition. Generally, `3` or more hidden layers qualifies a model as \"deep\". The hidden layers collectively form a *feature hierarchy* - advancing further in the network corresponds to increasing complexity and abstraction of the features it can recognize. This is great for large, high-dimensional data, e.g. raw media like photos. \n",
    "\n",
    "An MLP can be created with [MLPClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html), for which you can specify hidden layer size, activation function, weight optimization solver, batch size, verbosity, and whether training should stop early to avoid overfitting, and others.\n",
    "\n",
    "For this problem, we want to create an MLP classifier with at least `3` hidden layers. We also want to choose the activation function, which does the work of deciding whether a neuron should fire (or \"activate\") or not. At its simplest, this is a step function that fires the neuron if `y > threshold` and does not fire otherwise. For this problem, let's choose `ReLU`(Rectified Linear Units), which returns 0 if it receives negative input and for positive input, simply returns the value back. There's a lot more that can be said, so look [here](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning) if you're interested. \n",
    "\n",
    "Next is the `solver`, which decides how the weights are mathematically optimized. There are a few options which the sklearn [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) describes, but for simplicity, just stick with the default `adam` for now. \n",
    "\n",
    "Next is `batch_size` which designates the size of minibatches for stochastic optimizers. This just means training data is split into groups so that gradient descent can reduce the variance of the gradient by averaging the gradients of the groups.\n",
    "\n",
    "For our own sake, let's tell the model to print out how it's doing each epoch/iteration. This is just a flag we want to set with `verbose=True`. Finally, let's avoid overfitting by telling the model to stop if the validation score does not improve enough over consecutive epochs.\n",
    "\n",
    "**Parameters**\n",
    "- hidden_layer_sizes: a tuple of neurons per layer, default (100,)\n",
    "- activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’\n",
    "- solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’\n",
    "- batch_size: int, default 'auto'\n",
    "- verbose: boolean(True/False) for whether or not to print out progress at each stage of training.\n",
    "- early_stopping: boolean, prevents overfitting(like we did on the gradient descent assignment)\n",
    "\n",
    "\n",
    "**Procedure**\n",
    "- create the model object with at least 3 hidden layers and the appropriate parameters as detailed above.\n",
    "- fit the model on `x_train` and `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multilayer perceptron model\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,100,100), activation='relu', solver='adam',verbose=True, early_stopping=True)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now instead of getting hog features of entire image, break image into\n",
    "# patches and get hog features of each\n",
    "def sliding_window(img, patch_size=positive_patches[0].shape,\n",
    "                   istep=2, jstep=2, scale=1.0):\n",
    "    '''\n",
    "    '''\n",
    "    Ni, Nj = (int(scale * s) for s in patch_size)\n",
    "    for i in range(0, img.shape[0] - Ni, istep):\n",
    "        for j in range(0, img.shape[1] - Ni, jstep):\n",
    "            patch = img[i:i + Ni, j:j + Nj]\n",
    "            if scale != 1:\n",
    "                patch = transform.resize(patch, patch_size)\n",
    "            yield (i, j), patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hog_features(images: list, scale = 1.0):\n",
    "    image_hogs = []\n",
    "    image_indices = []\n",
    "    for image in images:\n",
    "        indices, patches = zip(*sliding_window(image, scale=scale))\n",
    "        image_indices.append(indices)\n",
    "        patches_hog = np.array([feature.hog(patch) for patch in patches])\n",
    "        image_hogs.append(patches_hog)\n",
    "        print(patches_hog.shape)\n",
    "    return image_hogs, image_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Image uploading (15 pts)\n",
    "You will be uploading at least `5` images in this portion - at least 1 must have a \"clearly visible\" face in it.\n",
    "<br>For this we will be using the Open Source Computer Vision(OpenCV) library. Specifically, we will be reading images in, manipulating and displaying them, then saving the changes.\n",
    "<br>The relevant functions here are `cv2.imread()`, `cv2.imshow()`, and `cv2.imwrite()`, and matplotlib's `plt.imshow()`\n",
    "<br>Before uploading, be sure each photo is a `jpg` and is relatively small (use your own or the provided images to begin with).\n",
    "\n",
    "**Procedure**\n",
    "- Make a list of filenames of your images\n",
    "- Create a list of targets with 1's corresponding to positive images and 0's for negatives (e.g., [1.0, 1.0, 1.0, 0.0, 0.0] would be positie for images 1, 2, and 3 and negative for the last two).\n",
    "- Loop over the filename list, call `cv2.imread()` on the filename with the `cv2.IMREAD_GRAYSCALE` option as the second parameter, and store the result in another list (e.g., images). \n",
    "- If you want to display the images, create another loop to go through the images and call `plt.imshow()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# image uploading\n",
    "filenames = [\"brain-1.jpg\", \"face-1.jpg\", \"face-2.jpg\", \"not-face-1.jpg\", \"not-face-2.jpg\"]\n",
    "# inputs\n",
    "images = []\n",
    "# targets\n",
    "images_targets = [0.0, 1.0, 1.0, 0.0, 0.0]\n",
    "\n",
    "# read\n",
    "for filename in filenames:\n",
    "    image = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "    images.append(image)\n",
    "\n",
    "# display\n",
    "for image in images:\n",
    "    plt.imshow(image, cmap = 'gray', interpolation = 'bicubic')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Rescale and show HOG features (15 pts)\n",
    "Loop over the list of images and using the helper functions in Problem 2 above, rescale each image and then display the hog features of each, again using the functions above (See the \"Example visualization of HOG features\" above for examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in images:\n",
    "    # rescale\n",
    "    cropped_image = rescale_img(convert_img_to_greyscale(image))[0:100, 70:160]\n",
    "    # hog features of inputs\n",
    "    show_hog_features(cropped_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Predict detections (20 pts)\n",
    "Use the MLP model to predict faces by feeding HOG patches to `predict()`.\n",
    "\n",
    "**Prodcedure**\n",
    "- call `get_hog_features()`, which returns two things: the HOG features of each patch and the indices of those patches.\n",
    "- loop over the HOG features variable, calling `predict()` on each. This result is the predicted labels - store in a `labels` varaible, then append each of those `labels` to a list (e.g., image_labels).\n",
    "- print how many patches have faces in them by calling `sum()` on `labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed patches to model to predict positive patches\n",
    "image_labels = []\n",
    "image_hogs, image_indices = get_hog_features(images, 1.0)\n",
    "for hog in image_hogs:\n",
    "    labels = model.predict(hog)\n",
    "    image_labels.append(labels)\n",
    "    print(sum(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7: plotting predictions  (10 pts)\n",
    "Finally, we want to visualize how the model did.\n",
    "- Loop over your images. On the `ith` iteration, call `show_patches()` with `images[i]`, `indices[i]`(this is the second value returned from `get_hog_features()`), and `labels[i]`(the list created in the previous problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(images), len(image_indices), len(image_labels))\n",
    "for i in range(len(images)):\n",
    "    show_patches(images[i], image_indices[i], image_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8: Improvements (10 pts)\n",
    "Weird results, right? How can we improve the model's performance? There are a lot of possibilities, but consider starting with sliding box scale. You can modify this by calling `pipeline()` below, which just does all the steps above.\n",
    "\n",
    "There isn't one right answer here, and your results don't necessarily have to improve, so explore and experiment with what affects performance.\n",
    "\n",
    "Some possibilities:\n",
    "- sliding box scale: make the sliding box/window bigger or smaller by passing different values to `pipeline()` below\n",
    "- normalzing data with StandardScaler, like in lecture 23\n",
    "- randomizing input data before splitting into training and testing sets. For this, make sure the targets list is shuffled in the same way so as to still coresspond to the inputs.\n",
    "\n",
    "Some possible extensions:\n",
    "- instead of plotting every box, averge their locations and plot only a single \"best guess\" for where the face(s) is(are)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(images: list, indices: list, labels: list):\n",
    "    for i in range(len(images)):\n",
    "        show_patches(images[i], indices[i], labels[i])\n",
    "\n",
    "# full workflow\n",
    "def pipeline(model, images: list, scale = 1.0):\n",
    "    image_labels = []\n",
    "    image_hogs, image_indices = get_hog_features(images, scale)\n",
    "\n",
    "    for hog in image_hogs:\n",
    "        labels = model.predict(hog)\n",
    "        image_labels.append(labels)\n",
    "\n",
    "    plot_predictions(images, image_indices, image_labels)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# try with scale 0.5, 0.75, 1.0, 1.25, 1.5\n",
    "pipeline(model, images, 0.5)\n",
    "pipeline(model, images, 0.75)\n",
    "pipeline(model, images, 1.0)\n",
    "pipeline(model, images, 1.25)\n",
    "pipeline(model, images, 1.5)\n",
    "\n",
    "#Hand to use above because kernel kept crashing with loop\n",
    "# for i in range(0.5, 1.5, 0.25):\n",
    "#     pipeline(model, images, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comments\n",
    "\"\"\"\n",
    "Time Spent: 2.5 hours\n",
    "Enjoyed:    This was a super interesting project with a satisfying result.\n",
    "Disliked:   Was hard to run on servers without kernel dying. I could get a\n",
    "            result without it crashing half the time. Much of the time spent\n",
    "            was waiting.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
