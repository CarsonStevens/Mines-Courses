{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 470 Activities and Case Studies\n",
    "\n",
    "1. For all activities, you are allowed to collaborate with a partner. \n",
    "1. For case studies, you should work individually and are **not** allowed to collaborate.\n",
    "\n",
    "By filling out this notebook and submitting it, you acknowledge that you are aware of the above policies and are agreeing to comply with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some considerations with regard to how these notebooks will be graded:\n",
    "\n",
    "1. You can add more notebook cells to test out or debug your code. We actually highly recommend you do so to gain a better understanding of what is happening. However, **during grading, these cells are ignored**.\n",
    "2. You must ensure that all your code for the particular task is available in the cells that say \"# YOUR CODE HERE\"\n",
    "3. Every cell that says \"# YOUR CODE HERE\" is followed by a \"raise NotImplementedError\". You need to remove that line. During grading, if an error occurs then you will not receive points for your work in that section.\n",
    "4. If your code passes the \"assert\" statements, then no output will result. If your code fails the \"assert\" statements, you will get an \"AssertionError\". Getting an assertion error means you will not receive points for that particular task.\n",
    "5. If you edit the \"assert\" statements to make your code pass, they will still fail when they are graded since the \"assert\" statements will revert to the original. Make sure you don't edit the assert statements.\n",
    "6. We may sometimes have \"hidden\" tests for grading. This means that passing the visible \"assert\" statements is not sufficient. The \"assert\" statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. Passing the visible tests is necessary but not sufficient to get the grade for that cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning - Regularization\n",
    "\n",
    "In this exercise we're going to look at how to lower the chance of overfitting by using regularization parameters with a variety of different linear models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso & Ridge Regression\n",
    "\n",
    "[Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) and [ridge regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html?highlight=ridge#sklearn.linear_model.Ridge) are two commonly used linear models with regularization. \n",
    "\n",
    "Lasso uses the $l_1$ norm for regularization and so has the following objective:\n",
    "\n",
    "$$ 1 / (2 * n_{samples})) * ||y - Xw||^2_2 + \\alpha * ||w||_1 $$ \n",
    "\n",
    "Whereas Ridge regression uses the $l_2$ norm for regularization and has the following objective:\n",
    "\n",
    "$$ ||y - Xw||^2_2 + \\alpha * ||w||^2_2 $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the 10x10 Hilbert matrix\n",
    "X = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])\n",
    "y = np.ones(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 200\n",
    "alphas = np.logspace(-10, -1, n_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we are building a model to predict the y values which are all 1's. We could easily do this with a linear model of all coefficients set to 0 and an intercept of 1. However, what we're trying to show is the power of the model with changing the coefficients with respect to the input variable and not using an intercept. \n",
    "\n",
    "In creating your model, make sure to set the **fit_intercept to False**. This will then allow you to more easily see the value in changing the $\\alpha$ values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb05a7ca0d7381286b05462057152a6b",
     "grade": false,
     "grade_id": "cell-95e2d022f434dcb4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def determine_coefficients(alphas, model_class, X, y):\n",
    "    \"\"\"Determine the coefficients of a linear model given the various alphas. \n",
    "    You should train a model for each value of alpha and then score it using mean_sqaure error\n",
    "    \n",
    "    Args:\n",
    "        alphas (iterable): The alphas to test out with the model\n",
    "        model_class (sklearn.estimator Class): A type of linear model **not instantiated**\n",
    "        X (iterable): The data to train on\n",
    "        y (iterable): The labels to train on\n",
    "        \n",
    "    Returns:\n",
    "        coefs (iterable): the coefficients extracted from the trained model. See model.coef_\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = determine_coefficients(alphas, Ridge, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9daa04d3180b34879edb82bdbf878c82",
     "grade": true,
     "grade_id": "cell-b61547d05006589c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert coefs\n",
    "assert coefs[0].shape == (10,)\n",
    "assert len(coefs) == len(alphas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge coefficients as a function of the regularization')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d1d4cee17b9ca9438e0c4bacdb00f18",
     "grade": false,
     "grade_id": "cell-0a84e3a5c8d04d75",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the coefficients of lasso with the same data and alphas as ridge to lassoCoefs\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2be4a9bd631a0cbfbc926f0d4278204",
     "grade": true,
     "grade_id": "cell-056c4c411e2ab8ab",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert lassoCoefs\n",
    "assert lassoCoefs[0].shape == (10,)\n",
    "assert len(lassoCoefs) == len(alphas)\n",
    "assert not np.all([lassoCoefs,coefs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(alphas, lassoCoefs)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Lasso coefficients as a function of the regularization')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with lasso, the coefficients are significantly smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "\n",
    "[Elastic net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html?highlight=elastic#sklearn.linear_model.ElasticNet) combines the two approaches by using both the $l_1$ and $l_2$ squared norms minimizing the objective:\n",
    "\n",
    "$$ 1 / (2 * n_{samples}) * ||y - Xw||^2_2\n",
    "+ \\alpha * l1_{ratio} * ||w||_1\n",
    "+ 0.5 * \\alpha * (1 - l1_{ratio}) * ||w||^2_2$$\n",
    "\n",
    "\n",
    "\n",
    "> The next section is copied from [the documentation](http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_coordinate_descent_path.html) and is meant to show you how the coefficients change with various values of alphe for both lasso and elastic net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from sklearn.linear_model import lasso_path, enet_path\n",
    "from sklearn import datasets\n",
    "\n",
    "eps = 5e-3\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps, fit_intercept=False)\n",
    "alphas_enet, coefs_enet, _ = enet_path(X, y, eps=eps, l1_ratio=0.8, fit_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "colors = cycle(['b', 'r', 'g', 'c', 'k'])\n",
    "neg_log_alphas_lasso = -np.log10(alphas_lasso)\n",
    "neg_log_alphas_enet = -np.log10(alphas_enet)\n",
    "for coef_l, coef_e, c in zip(coefs_lasso, coefs_enet, colors):\n",
    "    l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c)\n",
    "    l2 = plt.plot(neg_log_alphas_enet, coef_e, linestyle='--', c=c)\n",
    "\n",
    "plt.xlabel('-Log(alpha)')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title('Lasso and Elastic-Net Paths')\n",
    "plt.legend((l1[-1], l2[-1]), ('Lasso', 'Elastic-Net'), loc='lower left')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with elastic net, the coefficients go above zero with a larger value of alpha. Change the `l1_ratio` above and see how that affects the graph.\n",
    "\n",
    "The higher the `l1_ratio` the faster the coefficients should go above zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed936ab53a1391c5e6af8df699a1dbf5",
     "grade": false,
     "grade_id": "feedback",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def feedback():\n",
    "    \"\"\"Provide feedback on the contents of this exercise\n",
    "    \n",
    "    Returns:\n",
    "        string\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f39f6185a54850c2f1f9b5b2a17b7543",
     "grade": true,
     "grade_id": "feedback-tests",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml-exercises",
   "language": "python",
   "name": "iml-exercises"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
