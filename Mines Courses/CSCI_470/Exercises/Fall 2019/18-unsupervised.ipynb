{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### CSCI 303\n",
    "# Introduction to Data Science\n",
    "<p/>\n",
    "### 18 - Unsupervised Learning\n",
    "\n",
    "![PCA scatter plots](pca.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Lecture\n",
    "---\n",
    "- Introduction to unsupervised learning\n",
    "- Data preprocessing\n",
    "  - Scaling and normalization\n",
    "  - Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup\n",
    "---\n",
    "The obligatory setup code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# function for generating normally distributed data|\n",
    "def sample_cluster(n, x, y, sigma):\n",
    "    x = np.random.randn(n) * sigma + x;\n",
    "    y = np.random.randn(n) * sigma + y;\n",
    "    return np.array([x, y]).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised vs Supervised\n",
    "---\n",
    "In supervised learning, we have *labeled* data:\n",
    "- some input variables \n",
    "- some additional variable(s) which we are learning to predict\n",
    "\n",
    "For example, we might have a classification problem like the one below (colors = class labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "c1 = sample_cluster(50, 0, 0.5, 0.15)\n",
    "c2 = sample_cluster(50, 0, 1.2, 0.1)\n",
    "c3 = sample_cluster(50, 0.5, 1, 0.2)\n",
    "plt.plot(c1[:,0], c1[:,1], 'r.', c2[:,0], c2[:,1], 'b.', c3[:,0], c3[:,1], 'y.',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In *unsupervised* learning, we are given no labels, and we seek to find hidden patterns in the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(c1[:,0], c1[:,1], 'k.', c2[:,0], c2[:,1], 'k.', c3[:,0], c3[:,1], 'k.',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Questions we could ask about the data:\n",
    "\n",
    "- Is there a transformation of the data which will reveal patterns (to humans or algorithms)?\n",
    "- What are the relevant features of the data which are informative?\n",
    "- Are there natural groupings into which we could separate the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenges of Unsupervised Learning\n",
    "---\n",
    "Since we have no labeled data, there are no predictions that we can make *and meaningfully test*.\n",
    "\n",
    "Evaluation of unsupervised learning algorithms is often largely subjective.\n",
    "\n",
    "Unsupervised learning is often used in *exploratory data analysis*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example Applications\n",
    "---\n",
    "- group (cluster) gene expression data in cancer patients to look for patterns; a gene (or group of genes) which strongly differentiates patients may be worth further study:\n",
    "  - different disease causes\n",
    "  - different responses to treatment\n",
    "- look for anomalous patterns in credit card spending\n",
    "- group people or organizations according to some new identifiers\n",
    "  - reveal hidden similarities\n",
    "  - provide alerts to activities with similar risks (e.g., fund analysis)\n",
    "  - targeted marketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Preprocessing\n",
    "---\n",
    "- Generally useful to improve supervised learning algorithm performance\n",
    "- Scaling/normalization:\n",
    "  - Transform data so that features are on same scale or have same statistics\n",
    "  - Helps some algorithms which are sensitive to scale\n",
    "- Dimensionality reduction:\n",
    "  - Transform data into a sub-space in which visualization or learning is easier\n",
    "  - Reduce computational cost of learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scaling\n",
    "---\n",
    "Is a thing.  It helps with some algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimensionality Reduction\n",
    "---\n",
    "- Input data is often (very) high dimensional\n",
    "- This can lead to expensive learning and promotes overfitting\n",
    "- Variables can often also have high correlation\n",
    "- Solution: extract most relevant sub-space of input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Components Analysis\n",
    "---\n",
    "The most popular form of dimensionality reduction.\n",
    "\n",
    "Lots of linear algebra behind this.  We won't go there.\n",
    "\n",
    "Basically, rotates and transforms the data into a new subspace:\n",
    "- Most relevance (variation) is now associated with first feature\n",
    "- Second feature gets the next most, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## PCA Example\n",
    "---\n",
    "Consider this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [[1, -1, 7],[20, 3, -5],[1,1,1]]\n",
    "x1 = np.random.randn(300);\n",
    "y1 = np.random.randn(300);\n",
    "z1 = np.random.randn(300);\n",
    "data1 = np.array([x1, y1, z1]).T @ M\n",
    "x2 = np.random.randn(300);\n",
    "y2 = np.random.randn(300);\n",
    "z2 = np.random.randn(300);\n",
    "data2 = np.array([x2, y2, z2]).T @ M + np.array([20,-10,15])\n",
    "data = np.concatenate((data1, data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(data1[:,0], data1[:,1], data1[:,2])\n",
    "ax.scatter(data2[:,0], data2[:,1], data2[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's apply PCA and look at the first two principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)\n",
    "data1_pca = pca.transform(data1)\n",
    "data2_pca = pca.transform(data2)\n",
    "\n",
    "plt.scatter(data1_pca[:,0], data1_pca[:,1])\n",
    "plt.scatter(data2_pca[:,0], data2_pca[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Taiwan Credit Card Default Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('default.csv', header=1, encoding='utf8', index_col='ID')\n",
    "all_dummies = ['SEX', 'EDUCATION','MARRIAGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']\n",
    "df3 = pd.get_dummies(data, columns=all_dummies)\n",
    "target = 'default payment next month'\n",
    "inputs3 = df3.columns.drop(target)\n",
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X = df3[inputs3]\n",
    "t = df3[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#print(X)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "ss.fit(X)\n",
    "X_scaled = ss.transform(X)\n",
    "#print(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_scaled)\n",
    "X_pca = pca.transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalized = Normalizer()\n",
    "normalized.fit(X)\n",
    "X_norm = normalized.transform(X)\n",
    "#print(X_norm)\n",
    "X_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_norm)\n",
    "X_pca = pca.transform(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Time\n",
    "---\n",
    "- Clustering"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "height": 768,
   "start_slideshow_at": "selected",
   "theme": "mines",
   "transition": "slide",
   "width": 1024
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
