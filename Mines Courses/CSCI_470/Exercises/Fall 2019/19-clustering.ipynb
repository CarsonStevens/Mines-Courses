{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### CSCI 303\n",
    "# Introduction to Data Science\n",
    "<p/>\n",
    "### 19-Clustering\n",
    "\n",
    "![K-Means iterations](kmeans.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Lecture\n",
    "---\n",
    "- Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup\n",
    "---\n",
    "The obligatory setup code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# function for generating normally distributed data|\n",
    "def sample_cluster(n, x, y, sigma):\n",
    "    x = np.random.randn(n) * sigma + x;\n",
    "    y = np.random.randn(n) * sigma + y;\n",
    "    return np.array([x, y]).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Clustering Example\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "c1 = sample_cluster(100, 0, 0.5, 0.15)\n",
    "c2 = sample_cluster(100, 0, 1.2, 0.1)\n",
    "c3 = sample_cluster(100, 0.5, 1, 0.2)\n",
    "data = np.concatenate((c1, c2, c3))\n",
    "plt.plot(data[:,0], data[:,1], 'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering\n",
    "---\n",
    "Simple idea:\n",
    "\n",
    "- Choose $K$ (decide how many clusters you *think* there ought to be)\n",
    "- Partition the data into $K$ disjoint clusters such that $\\sum_{k=1}^K W(C_k)$ is minimized.\n",
    "  - W(C) is some measure of within-cluster variation\n",
    "  - Common choice for W is the sum of Euclidean distances between pairs of points:\n",
    "  \n",
    "  $$ W(C_k) = \\frac{1}{|C_k|} \\sum_{pairs(x,y): x, y \\in C_k, x \\ne y} \\|x - y\\|^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution Approaches\n",
    "---\n",
    "- Brute force: try every possible partitions of the data\n",
    "  - Advantage: globally optimal\n",
    "  - Disadvantage: $\\approx K^n$ different partitions (given $n$ data points)\n",
    "- K-Means algorithm: iterative improvement\n",
    "  - Advantage: efficient\n",
    "  - Disadvantage: locally optimal, result depends on initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means Algorithm\n",
    "---\n",
    "- Various initialization schemes\n",
    "  - One way is to assign each point a cluster identity at random\n",
    "  - Another is to choose K points to serve as initial cluster centers at random\n",
    "- Iterate until cluster assignments stop changing:\n",
    "  - Compute the cluster *centroids*.  The centroid is the mean point (vector of means of all features) for all points in the cluster\n",
    "  - Re-assign each data point to the cluster associated with the nearest centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This algorithm is ridiculously easy to code up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple k-means implementation\n",
    "# input: integer K, ndarray data\n",
    "def KMeans(K, data):\n",
    "    n = data.shape[0]\n",
    "    # intialize data into random clusters\n",
    "    c = np.random.randint(low=0, high=K, size=n)\n",
    "    C = [c]\n",
    "    changed = True\n",
    "    centroid = lambda k: np.mean(np.array([d for c,d in zip(c,data) if c == k]), axis=0)\n",
    "    while changed:\n",
    "        centroids = np.array([centroid(k) for k in range(K)])\n",
    "        dists = [[np.linalg.norm(d - centroids[k]) for k in range(K)] for d in data]\n",
    "        newc = np.argmin(np.array(dists), axis=1)\n",
    "        changed = np.any(c != newc)\n",
    "        C.append(newc)\n",
    "        c = newc\n",
    "    return C      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's give it a spin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = KMeans(4, data)\n",
    "print(\"Iterations:\", len(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's look at the first few iterations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap(['r', 'y','b'])\n",
    "plt.figure(figsize=[20,5])\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.scatter(data[:,0], data[:,1], c=C[i], cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Look at the final four:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20,5])\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.scatter(data[:,0], data[:,1], c=C[-(4-i)], cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's an implementation that picks K points at random and clusters on those points initially:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple k-means implementation\n",
    "# input: integer K, ndarray data\n",
    "def KMeans2(K, data):\n",
    "    n = data.shape[0]\n",
    "    # intialize data using random centers\n",
    "    centroids = data[np.random.randint(low=0, high=n, size=3)]\n",
    "    dists = [[np.linalg.norm(d - centroids[k]) for k in range(K)] for d in data]\n",
    "    c = np.argmin(np.array(dists), axis=1)\n",
    "    C = [c]\n",
    "    changed = True\n",
    "    centroid = lambda k: np.mean(np.array([d for c,d in zip(c,data) if c == k]), axis=0)\n",
    "    while changed:\n",
    "        centroids = np.array([centroid(k) for k in range(K)])\n",
    "        dists = [[np.linalg.norm(d - centroids[k]) for k in range(K)] for d in data]\n",
    "        newc = np.argmin(np.array(dists), axis=1)\n",
    "        changed = np.any(c != newc)\n",
    "        C.append(newc)\n",
    "        c = newc\n",
    "    return C      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "C = KMeans2(3, data)\n",
    "print(\"Iterations:\", len(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First few interations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20,5])\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.scatter(data[:,0], data[:,1], c=C[i], cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Final four:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20,5])\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.scatter(data[:,0], data[:,1], c=C[-(4-i)], cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Of course, scikit-learn gives us a robust KMeans object.\n",
    "\n",
    "It behaves just like a classifier, except that you only give it inputs (no class labels).\n",
    "\n",
    "The scikit-learn KMeans clusterer will actually try several random starts, and pick the \"best\" result.\n",
    "\n",
    "Unfortunately, it doesn't give us a way to visualize the steps it went through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(3)\n",
    "km.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], c=km.predict(data), cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Guessing K\n",
    "---\n",
    "I know the data used above was generated from 3 Gaussian blobs, so we've been using K.\n",
    "\n",
    "In general, though, how many clusters should there be?\n",
    "\n",
    "How can we guess?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try K=2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km2 = KMeans(2)\n",
    "km2.fit(data)\n",
    "plt.scatter(data[:,0], data[:,1], c=km2.predict(data), cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or K=4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = ListedColormap(['r','y','b','m'])\n",
    "km4 = KMeans(4)\n",
    "km4.fit(data)\n",
    "plt.scatter(data[:,0], data[:,1], c=km4.predict(data), cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or K=10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km10 = KMeans(10)\n",
    "km10.fit(data)\n",
    "plt.scatter(data[:,0], data[:,1], c=km10.predict(data), cmap='nipy_spectral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Clustering\n",
    "---\n",
    "One answer to the problem of \"how many clusters\":\n",
    "\n",
    "Try all of them!\n",
    "\n",
    "Hierarchical algorithms are either top down or bottom up.\n",
    "\n",
    "The bottom up algorithms (agglomerative) are the most popular.\n",
    "\n",
    "The result of hierarchical clustering is a *dendrogram* plot, showing how the clusters break down (or build up, if you prefer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's first get a smaller dataset to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = sample_cluster(10, 0, 0.5, 0.05)\n",
    "c2 = sample_cluster(10, 0, 1.2, 0.1)\n",
    "c3 = sample_cluster(10, 0.5, 1, 0.05)\n",
    "data = np.concatenate((c1, c2, c3))\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While scikit-learn has agglomerative clustering, it isn't capable of showing us a dendrogram.\n",
    "\n",
    "We'll use SciPy instead for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "linkage_array = ward(data)\n",
    "plt.figure(figsize=[20,5])\n",
    "dendrogram(linkage_array)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "height": 768,
   "start_slideshow_at": "selected",
   "theme": "mines",
   "transition": "slide",
   "width": 1024
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
