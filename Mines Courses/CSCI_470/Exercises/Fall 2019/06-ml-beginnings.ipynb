{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### CSCI 303\n",
    "# Introduction to Data Science\n",
    "<p/>\n",
    "### 6 - Machine Learning Beginnings\n",
    "\n",
    "![a fitted curve](fit.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Looking Ahead\n",
    "---\n",
    "### Tentative Plan\n",
    "- We've learned just some basics of Python\n",
    "  - We left out a lot!\n",
    "    - Functions, modules, classes\n",
    "    - NumPy, pandas, matplotlib\n",
    "  - We will learn *some* of these in context of rest of class\n",
    "- Next task is to start on some machine learning\n",
    "  - Provide context for learning additional technologies\n",
    "  - Introduce basic concepts we'll build on later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Lecture\n",
    "---\n",
    "- Introduce basic supervised learning concepts\n",
    "- Introduce NumPy & matplotlib\n",
    "- Introduce linear regression\n",
    "- Explore linear regression using NumPy and matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine (or Statistical) Learning\n",
    "---\n",
    "- Learning structure and relationships in data\n",
    "- Supervised learning\n",
    "  - Goal is predicting outputs based on inputs\n",
    "  - Learn from labeled exemplar data\n",
    "  - E.g., regression, classification\n",
    "- Unsupervised learning\n",
    "  - Goal is reveal hidden structure in the data\n",
    "  - Inputs but no labeled outputs\n",
    "  - E.g., clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "---\n",
    "Motivation: consider a relatively simple prediction problem\\*:\n",
    "  - Inputs: advertising dollars spent (in various media)\n",
    "  - Outputs: total sales\n",
    "  - Problem: predict sales for new ad buy\n",
    "\n",
    "\\*This example comes from D. James, et al., *An Introduction to Statistical Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terms\n",
    "---\n",
    "The *inputs* in this case are numerical values:\n",
    "\n",
    "- Advertising dollars (for some given week)\n",
    "  - TV\n",
    "  - Radio\n",
    "  - Newspaper\n",
    "\n",
    "Inputs are often denoted as $X$, and go by various names:\n",
    "\n",
    "- predictors\n",
    "- independent variables\n",
    "- features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *outputs* are also numerical values in a regression problem:\n",
    "\n",
    "- Total sales (for some given week)\n",
    "\n",
    "The outputs are typically labeled $Y$, and are called:\n",
    "\n",
    "- response variables\n",
    "- dependent variables\n",
    "- targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Model\n",
    "---\n",
    "In regression, we typically assume the existence of some hidden function $f$, such that\n",
    "\n",
    "$$ Y = f(X) + \\epsilon $$\n",
    "\n",
    "where $\\epsilon$ represents a random error or *noise* term with zero mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction\n",
    "---\n",
    "Given some data (e.g., historical sales data) that includes both the inputs and the outputs, can we make an informed guess of the output for some (previously unseen) input?\n",
    "\n",
    "We write\n",
    "\n",
    "$$ \\hat Y = \\hat f (X) $$\n",
    "\n",
    "where the \"hat\" on $\\hat Y$ and $\\hat f$ means these are approximate.\n",
    "\n",
    "Essentially, since $\\epsilon$ has zero mean, the best approximation we can obtain for some unobserved output $y'$ associated with inputs $x'$ is $f(x')$.  However, since $f$ itself is unavailable, we'll use an *estimate* of $f$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parametric Approximation\n",
    "---\n",
    "- There are different kinds of approximation techniques for estimating $f$.\n",
    "- We'll study these in more depth later\n",
    "- For now, focus on *parametric* approximation, in which:\n",
    "  - $\\hat f(X) = f(X, \\theta)$\n",
    "  - $\\theta$ is a vector of *parameters* of some model of $f$\n",
    "- More specifically, we'll look at *linear regression*:\n",
    "  - $\\hat f(X) = f(X, \\theta) = \\theta_0 + \\theta_1 X_1 + \\theta_2 X_2 + ... + \\theta_k X_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notation for Linear Regression\n",
    "---\n",
    "\n",
    "Let's rewrite a bit: given an input (which we assume to be a vector of inputs: $\\mathbf{x}$)\n",
    "\n",
    "- let $\\phi$ represent the vector $(1, x_1, x_2, ..., x_k)$\n",
    "- instead of $\\theta$ for our parameter vector, we'll use $\\mathbf{w}$\n",
    "- Then \n",
    "\n",
    "$$\\begin{align}\n",
    "    \\hat f(\\mathbf{x}) & = 1 w_0 + x_1 w_1 + ... + x_k w_k \\\\\n",
    "                   & = \\phi \\cdot \\mathbf{w}\n",
    "  \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some Linear Algebra\n",
    "---\n",
    "Now suppose we have $n$ examples.\n",
    "\n",
    "We stack all of our $\\phi$ vectors as rows in the matrix $\\Phi$:\n",
    "\n",
    "$$ \\Phi = \\left[ \\begin{array}{cccc} \\phi_{00} & \\phi_{01} & \\dots & \\phi_{0k} \\\\\n",
    "                                     \\phi_{10} & \\phi_{11} & \\dots & \\phi_{1k} \\\\\n",
    "                                     \\vdots & \\ddots & & \\vdots \\\\\n",
    "                                     \\phi_{n0} & \\phi_{n1} & \\dots & \\phi_{nk} \\end{array}\n",
    "          \\right] $$\n",
    "          \n",
    "$\\Phi$ is often called the *design matrix*.\n",
    "                                     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We similarly take all of our $n$ outputs $y_i$ to make a vector $\\mathbf{y}$:\n",
    "\n",
    "$$ \\mathbf{y} = \\left[ \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array} \\right] $$\n",
    "\n",
    "$\\mathbf{y}$ is the *target vector*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then we are interested in finding vector $\\mathbf{w}$ which as close as possible satisfies the equation\n",
    "\n",
    "$$ \\Phi \\mathbf{w} = \\mathbf{y} $$\n",
    "\n",
    "For the \"ordinary least squares\" (**OLS**) solution, we find\n",
    "\n",
    "$$ \\mathbf{w} = \\arg \\min_{\\mathbf{w'}} \\| \\mathbf{y} - \\Phi \\mathbf{w'} \\| $$\n",
    "\n",
    "which, it turns out, can be solved for using linear algebra to obtain\n",
    "\n",
    "$$ \\mathbf{w} = (\\Phi^T \\Phi)^{-1} \\Phi^T \\mathbf{y} $$\n",
    "\n",
    "$\\mathbf{w}$ is the *weight vector*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Algebra in Python: NumPy\n",
    "---\n",
    "- Like MATLAB, NumPy provides wrappers onto specialized linear algebra libraries written in C/Fortran\n",
    "- NumPy adds to Python a basic type: `ndarray`\n",
    "  - Flexible, multidimensional array type\n",
    "  - Supports scalar and vector/matrix math operations\n",
    "  - Easily converts to/from Python sequence types\n",
    "    - However, `ndarray` imposes container typing\n",
    "- See the docs (Help menu in Jupyter notebook) for more info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OLS Example Using NumPy and matplotlib\n",
    "---\n",
    "For the rest of this lecture, we're going to:\n",
    "\n",
    "- Generate (simulate) a regression problem (NumPy)\n",
    "- Perform OLS regression on the problem (NumPy)\n",
    "- Plot results (matplotlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Generate a Problem\n",
    "---\n",
    "First, we need a function to be our \"unknown\" function to learn.\n",
    "\n",
    "We're going to stick with polynomials in one variable for this example.\n",
    "\n",
    "How about:\n",
    "\n",
    "$$ f(x) = 3 + 0.5 n - n^2 + 0.15 n^3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's visualize this function.  \n",
    "\n",
    "First, we need some points along the curve.\n",
    "\n",
    "To do that, we need the NumPy library *imported* into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    # use np as an alias by convention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use NumPy's `ndarray` type and generate some inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(-5, 5, 0.1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, compute f(x) for each input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = 3 + 0.5 * X - X**2 + 0.15 * X**3\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that we could simply do the math on the `ndarray` object.\n",
    "\n",
    "By default, all math operations on ndarrays is element-wise.\n",
    "\n",
    "Vector operations require other notation (as we'll see)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's plot our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # another convention\n",
    "plt.plot(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Great!  Now, our model is\n",
    "\n",
    "$$ Y = f(X) + \\epsilon $$\n",
    "\n",
    "and we assume we only have some data to guide us.\n",
    "\n",
    "Let's generate some random inputs, then compute Y by adding random noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, we'll make use of `np.random.random(n)`, which generates an array of $n$ points in the interval [0, 1).\n",
    "\n",
    "We have to scale and translate the points into the input range of interest - [-5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.random.random(20) * 10 - 5\n",
    "trainX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, compute $f(x)$ plus some zero mean Gaussian noise on each sample input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = 3 + 0.5 * trainX - trainX**2 + 0.15 * trainX**3  # f(x)\n",
    "plt.plot(trainX, trainY, 'k.')\n",
    "plt.show()\n",
    "\n",
    "trainY = trainY + (np.random.randn(trainY.size) * 3)     # noise\n",
    "\n",
    "plt.plot(trainX, trainY, 'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Adding the true function in, we can see better the noise term in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainX, trainY, 'k.')\n",
    "plt.plot(X, Y, 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yay! Step 1 complete!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Linear Regression\n",
    "---\n",
    "For this, we'll need some *features* to make $\\Phi$\n",
    "\n",
    "- X data of the type we'd have as inputs\n",
    "- We'll cheat and use polynomials of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Phi = np.array([trainX ** p for p in range(6)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's examine this a bit further.\n",
    "\n",
    "To create a matrix in NumPy, use `np.array` on a sequence of sequences (rows):\n",
    "\n",
    "E.g., the array\n",
    "\n",
    "$$ A = \\left[ \\begin{array}{ccc} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{array} \\right] $$\n",
    "\n",
    "is created with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2,3],\n",
    "              [4,5,6]])\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = np.array([trainX ** p for p in range(6)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Note the list comprehension applying exponentials to *ndarray* objects!\n",
    "\n",
    "This generates rows of data which are powers of the training X inputs.\n",
    "\n",
    "Finally, the `.T` at the end transposes the matrix so that these rows become columns as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "OK, we're ready to perform regression using OLS:\n",
    "\n",
    "Remember, we want to obtain\n",
    "\n",
    "$$ \\mathbf{w} = (\\Phi^T \\Phi)^{-1} \\Phi^T \\mathbf{y} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linalg.inv(Phi.T @ Phi) @ Phi.T @ trainY\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!  Remember, our true coefficients are 3, 0.5, -1, 0.15 and zeroes for the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Plot Results\n",
    "---\n",
    "Let's use our learned weights to estimate (predict) Y across our range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = np.array([X ** p for p in range(6)]).T @ w\n",
    "plt.plot(X, Yhat, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Looks promising!  Let's see it against the sample data and the true function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, Yhat, 'r-', X, Y, 'b-', trainX, trainY, 'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next Steps\n",
    "---\n",
    "In the next lecture, we'll:\n",
    "- We will continue exploring a few more machine learning concepts using our example\n",
    "- Talk more about NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "height": 768,
   "start_slideshow_at": "selected",
   "theme": "mines",
   "transition": "slide",
   "width": 1024
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
