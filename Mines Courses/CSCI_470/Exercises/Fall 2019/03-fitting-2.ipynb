{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 : NumPy, matplotlib, and Python functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Description\n",
    "\n",
    "Below is a guided exercise in doing linear regression using a gradient descent algorithm.  The goal here isn't to teach you the gradient descent algorithm, or scientific computing in general, but to give you experience with using NumPy for matrix-vector math, matplotlib for simple visualizations, and more Python (specifically, writing functions).\n",
    "\n",
    "### Grading\n",
    "\n",
    "For grading purposes, we will clear all outputs from all your cells and then run them all from the top.  ***Please* test your notebook in the same fashion before turning it in.**\n",
    "\n",
    "### Submitting Your Solution\n",
    "\n",
    "To submit your notebook, first clear all the cells (this won't matter too much this time, but for larger data sets in the future, it will make the file smaller). Do this with `Cell->All Output->Clear`.Then use the `File->Download As->Notebook` to obtain the notebook file.  Finally, submit the notebook file on Canvas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Code\n",
    "\n",
    "Feel free to look through this and see what it is doing, but basically it is just setting up a synthetic regression problem for you, very much like we did in class.  Just make sure you execute the cell before going on to the problems!  If you wish to see the training and testing points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def f(X):\n",
    "    return 1/(1 + np.exp(-X))\n",
    "\n",
    "n = 50\n",
    "np.random.seed(12345)\n",
    "trainX = np.random.random(n) * 10 - 5\n",
    "trainY = f(trainX) + np.random.randn(n) * 0.125\n",
    "testX = np.random.random(n) * 10 - 5\n",
    "testY = f(testX) + np.random.randn(n) * 0.125\n",
    "plt.plot(trainX, trainY, 'b.', testX, testY, 'gx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Problem 1: Features (10 points)\n",
    "For this problem, you need to create a Python function named `phi` that will produce a set of between `5` and `8` features for a given `X` input, such as the `trainX` vector created above.  This is basically what we did in lectures 5-7 to generate our design matrix $\\Phi$.  The difference is that in our lectures we used powers of $x$ exclusively, where as below I would like you to use at least two non-polynomial functions of $x$.  You can use any functions you want; consider:\n",
    "- trigonometric\n",
    "- logarithmic\n",
    "- exponential\n",
    "- radial basis function (Gaussians)\n",
    "- etc.  \n",
    "\n",
    "**The only functions you are forbidden to use are functions generating a sigmoid** such as the one generating the synthetic data for this problem (this includes the arctangent function)!  It is *recommended, but not required, that one of your features is an intercept term (all 1's).*\n",
    "\n",
    "Note the triple-quoted string at the top of the function - this produces a *documentation comment* for the function (visible when you do help(function) or function?).  You should get in the habit of making these for your code.\n",
    "\n",
    "Note: We discussed a variation of this in the `06-ml-beginnings` lecture ... your code will just have to *create a list containing different functions of x* to send into the `np.array()` instead of using all powers of $x$ like we did for `trainX` in the example: \n",
    "<br><br>`Phi = np.array([trainX ** p for p in range(6)]).T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(X):\n",
    "    '''\n",
    "    Return a design matrix (a NumPy ndarray) whose columns are functions of the input vector, X.\n",
    "    '''\n",
    "    # YOUR CODE HERE - FINISH THIS\n",
    "    order1 = (np.cos(X))*X\n",
    "    order2 = (np.sin(X))*X\n",
    "    order3 = np.log(X+10)*X\n",
    "    order4 = X*X*0.1 + 5\n",
    "    order5 = (np.cos(X) + np.sin(X))*X\n",
    "    phi = np.array([order1, order2, order3, order4, order5]).T\n",
    "    #print(phi)\n",
    "    return phi\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should probably check the output of phi(trainX) below, just to make sure none of your functions generated NaNs (Not a Number values) or infs (infinities) - these can result, e.g, from taking the log of a negative number or division by a number close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calls your newly created phi function above for testing\n",
    "\n",
    "phi = phi(trainX)\n",
    "phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2: Gradient Descent (20 points)\n",
    "The basic idea behind [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) is that you can *minimize a parametrized function* by simply taking small steps in the negative direction of the derivative of the function.  I'm not going to go into too many details here, since [Wikipedia's article](https://en.wikipedia.org/wiki/Gradient_descent) does such a good job.  Also see [Andrew Ng's class notes](https://datajobs.com/data-science-repo/Generalized-Linear-Models-[Andrew-Ng].pdf) on the topic for a more thorough mathematical treatment.\n",
    "\n",
    "In our cases, we are interesting in *minimizing the square error* in our linear approximation with respect to our training data.  That is, we want to minimize\n",
    "$$ \\begin{align}\n",
    "     & \\sum_{i=0}^n (\\hat{f}(x_i) - y_i)^2 \\\\\n",
    "   = & \\sum_{i=0}^n (\\phi_0(x_i) w_0 + \\phi_1(x_i) w_1 + \\dots + \\phi_k(x_i) w_k - y_i)^2 \\\\ \n",
    "   = & \\|\\Phi w - \\textbf{y}\\|^2 \n",
    "   \\end{align}\n",
    "$$\n",
    "\n",
    "Since all of our training data (the $x_i$ and $y_i$ terms) are known, the only variables we can optimize on are the $w$ variables. The *gradient* is (skipping over a *lot* of linear algebra):\n",
    "\n",
    "$$ \\begin{align}\n",
    "       & \\nabla \\|\\Phi w - \\textbf{y}\\|^2  \\\\\n",
    "     = & \\Phi^T (\\Phi w - \\textbf{y})\n",
    "   \\end{align}\n",
    "$$\n",
    "\n",
    "The gradient descent algorithm asks us, then, to start with some guess for $\\textbf{w}$ and repeatedly subtract some small multiple of the gradient from it, until $\\textbf{w}$ converges:\n",
    "\n",
    "$$ \\textbf{w} = \\textbf{w} + \\alpha \\Phi^T (\\textbf{y} - \\Phi \\textbf{w}) $$\n",
    "\n",
    "*Convergence* is a somewhat tricky issue, and the step size $\\alpha$ is a key to getting good convergence\n",
    "- a large $\\alpha$ speeds up the algorithm, but can miss the optimum by stepping too far (and can even diverge)\n",
    "- a small $\\alpha$ may result in a better optimum, but at the cost of speed.\n",
    "\n",
    "You may have to play around a bit with different values for $\\alpha$ until you get a good result; *print out the norm of the change in `w`* at each step to see if you are converging or diverging, or put in an `if` statement to *kill the function when the norm starts growing large*.  In testing this assignment, I found I had to use a pretty small $\\alpha$ to prevent divergence; 0.00001 worked for me.  However, you will get different results depending on your features.\n",
    "\n",
    "So, you will write a function `lr` which takes in $\\Phi$ (generated by your `phi` function above applied to `trainX`) and $y$ (`trainY`) and *returns a vector `w` computed by gradient descent*.  It is easiest to start `w` at the zero vector and go from there; as we already said, use $\\alpha = 0.1$.  Stop training when the norm of the change in `w` is less than `1e-6` (the norm of a vector is the square root of the dot product of the vector with itself) or in any case after `100,000` steps have been taken.  Before returning, your function should *print out the number of iterations (gradient descent steps) taken.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr(Phi, y):\n",
    "    '''\n",
    "    Computes a linear regression result w minimizing ||Phi w - y||^2 via gradient descent.\n",
    "    '''  \n",
    "    # YOUR CODE HERE - FINISH THIS\n",
    "    w = np.array([0] * Phi.shape[1])\n",
    "    #print(w.shape, Phi.shape, y.shape, np.transpose(Phi).shape, (np.dot(Phi,w)).shape)\n",
    "    a = 0.000001\n",
    "    norm = 1\n",
    "    steps = 0\n",
    "    while( norm > 0.000001):\n",
    "        steps = steps + 1\n",
    "        w = w + a * (np.transpose(Phi)) @ (y - Phi @ w)\n",
    "        norm = np.linalg.norm(a * (np.transpose(Phi)) @ (y - Phi @ w))\n",
    "        \n",
    "        #Kill Function\n",
    "        if (steps > 100000):\n",
    "            print(\"Steps hit 100,000. Gradient Descent Ended\")\n",
    "            return w;\n",
    "        \n",
    "    print(\"Number of gradient descent steps taken:\\t\", steps)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3: Compute RMSE (10 points)\n",
    "Use your `lr` function above to compute a solution, then using your solution, compute the RMSE of your result on **both the training and the test data sets, and print them below.** (For reference, I was able to achieve `0.11574` with `8` features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import math\n",
    "\n",
    "w = lr(phi, trainY)\n",
    "diff = trainY - (phi @ w)\n",
    "MSE = (diff ** 2).sum() / n\n",
    "RMSE_train = math.sqrt(MSE)\n",
    "print(\"RMSE trainY: \", RMSE_train, \"\\n\\n\")\n",
    "\n",
    "w = lr(phi, testY)\n",
    "diff = testY - (phi @ w)\n",
    "MSE = (diff ** 2).sum() / n\n",
    "RMSE_test = math.sqrt(MSE)\n",
    "print(\"RMSE testY: \", RMSE_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4: Plot Results (10 points)\n",
    "You are going to create `3` plots below. \n",
    "<br><br>On the first plot, **show the training and testing points** (using markers of different colors), and a line plot of your learned function (compute $\\hat{y}$ for $x \\in (-5,5)$ at increments of 0.1).  This can be accomplished using `plt.plot(...)`, following the examples in the lectures.  Don't forget you can also look at the matplotlib documentation - there's a link under the Help menu in your notebook.  If you feel especially daring, you can try doing the plot in [bokeh](http://bokeh.pydata.org/) instead.\n",
    "- To layer plots on top of each other, call `plt.plot()` multiple times before calling `plt.show()`\n",
    "\n",
    "Your second and third plots should be of the **residuals on the training set**.\n",
    "- The residuals are the errors, or differences between the training $y$ values and the corresponding $\\hat{y}$ values.\n",
    "\n",
    "There are various ways to look at the residuals; for the second plot, use `plt.scatter(...)` (or *bokeh* equivalent) to plot the training $x$ values versus the residuals for the training $y$ values.  If the values are roughly equally distributed above and below the $x$ axis, then our approximation was probably pretty good.\n",
    "\n",
    "The third plot should use `plt.hist(...)` (or *bokeh* equivalent) to show a **histogram** of the residuals (use `5` bins or so).  Given the small number of samples, it is unlikely you'll see a very normal distribution, but it should be vaguely normal, assuming we nearly approximated $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hint, you will need something like this (feel free to change), to get the x∈(−5,5) at increments of 0.1\n",
    "x = np.arange(-5,5,0.2)\n",
    "# YOUR CODE HERE - FINISH THIS\n",
    "\n",
    "# Plot 1: Plots all learned functions, Training points, and Test points\n",
    "learned = phi @ w\n",
    "plt.plot(trainX, learned, 'b-', trainX, trainY, 'k.', testX, testY, 'g.')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"X VS Y\")\n",
    "plt.legend([trainY, trainY, testY], labels=[\"Learned\", \"Train\", \"Test\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot2\n",
    "\n",
    "residuals = []\n",
    "residuals.append(trainY-testY)\n",
    "plt.scatter(trainX, residuals)\n",
    "plt.xlabel(\"trainX\")\n",
    "plt.ylabel(\"trainY - testY\")\n",
    "plt.title(\"trainX VS residuals\")\n",
    "plt.show()\n",
    "\n",
    "# Plot3\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = ax.hist(residuals, 5)\n",
    "ax.set_xlabel('Residual Value')\n",
    "ax.set_ylabel('Number of Values')\n",
    "ax.set_title(r'Histogram of Residuals')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
